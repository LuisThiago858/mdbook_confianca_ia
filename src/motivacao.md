# Motivação

A rápida difusão de sistemas baseados em Inteligência Artificial (IA) em contextos sensíveis — como saúde, finanças, educação e segurança — transformou esses artefatos em participantes ativos de decisões humanas. No entanto, essa transformação não foi acompanhada por um avanço proporcional nas formas de tornar a IA **compreensível, transparente e confiável para o público em geral** (SARGENT et al., 2022; UNESCO, 2021).

Diversos estudos apontam que usuários não especialistas, como estudantes, consumidores ou cidadãos comuns, frequentemente interagem com sistemas de IA sem compreender suas lógicas internas ou critérios de decisão (AFROOGH et al., 2024; LIAO & SUNDAR, 2022). Isso os coloca em uma posição de **vulnerabilidade cognitiva**, semelhante ao que ocorre com outros grupos hipervulneráveis no meio digital, como os idosos (GUEDES et al., 2023; MENDONÇA et al., 2024).

Essa **vulnerabilidade à opacidade algorítmica** não se restringe à falta de conhecimento técnico. Ela é ampliada por decisões de design que priorizam eficiência ou persuasão, em detrimento da explicabilidade, controle e participação do usuário. Assim como interfaces digitais podem induzir idosos ao erro ou à adesão acrítica, os sistemas de IA podem **conduzir o usuário a confiar excessivamente ou, ao contrário, rejeitar injustificadamente suas recomendações**, resultando em prejuízos pessoais ou sociais (KIESLICH et al., 2022; BACH et al., 2024).

Além disso, **a confiança não é neutra**: ela é construída por fatores técnicos (explicabilidade, acurácia), sociais (percepção de justiça, responsabilidade) e afetivos (experiências prévias, empatia com o sistema). Quando ignorados, esses fatores agravam a sensação de alienação e risco — especialmente em populações sem formação técnica — e dificultam a adoção ética e informada da IA.

Diante desse cenário, entender **por que usuários confiam (ou não) em sistemas de IA**, e **como essa confiança pode ser comprometida ou reforçada**, não é apenas uma questão técnica, mas social e ética. Assim como no caso da população idosa, urge investigar os mecanismos que ampliam a vulnerabilidade dos usuários e identificar estratégias de proteção, educação e *design* centrado no humano. A Engenharia de Requisitos, ao integrar diretrizes que consideram aspectos éticos, sociais e técnicos da IA, torna-se um campo fundamental para enfrentar esses desafios e reequilibrar a relação entre humanos e sistemas inteligentes (DAUN et al., 2022; BATISTA et al., 2024).
